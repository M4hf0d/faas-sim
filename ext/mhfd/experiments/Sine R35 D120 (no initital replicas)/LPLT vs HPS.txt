=== RUNNING HYPOTHESIS TEST ===
Hypothesis: LPLT reduces power consumption vs HPST baseline, but increases response times

=== POWER CONSUMPTION COMPARISON ===
 TOTAL INFRASTRUCTURE POWER:
HPST average total system power: 384.3W
LPLT average total system power: 392.4W
Infrastructure power savings: -2.1%

=== PERFORMANCE COMPARISON ===
HPST MEDIAN response time: 5668.118s
LPLT MEDIAN response time: 10174.578s
MEDIAN performance penalty: 79.5%
HPST 95th percentile: 146899.316s
LPLT 95th percentile: 376567.043s
95th percentile penalty: 156.3%
HPST warm-up avg: 30078.119s
LPLT warm-up avg: 81394.891s
Warm-up performance penalty: 170.6%

=== WAIT TIME ANALYSIS ===
HPST median wait time: 349.316s
LPLT median wait time: 9129.960s
Wait time improvement: -2513.7%

=== REVISED HYPOTHESIS RESULT ===
‚ùå HYPOTHESIS NOT CONFIRMED
   Power savings: -2.1%

============================================================
üîã REAL INFRASTRUCTURE POWER ANALYSIS (FIXED)
==================================================
HPST power data: 72600 measurements
LPLT power data: 72600 measurements
HPST unique nodes: 120
LPLT unique nodes: 120
HPST time range: 1.0 - 605.0
LPLT time range: 1.0 - 605.0

üìä  SYSTEM POWER:
HPST average total system power: 384.3W
LPLT average total system power: 392.4W
Infrastructure power savings: -2.1%

üìä POWER DISTRIBUTION:

HPST power by node type:
  coral: 4 nodes, avg 2.5W, total 9.8W
  nano: 29 nodes, avg 2.0W, total 57.7W
  nuc: 3 nodes, avg 8.7W, total 26.2W
  nx: 14 nodes, avg 7.9W, total 111.1W
  rockpi: 34 nodes, avg 3.6W, total 121.7W
  rpi3: 36 nodes, avg 1.6W, total 57.9W

LPLT power by node type:
  coral: 4 nodes, avg 2.5W, total 10.1W
  nano: 29 nodes, avg 2.0W, total 57.8W
  nuc: 3 nodes, avg 9.8W, total 29.3W
  nx: 14 nodes, avg 8.0W, total 111.4W
  rockpi: 34 nodes, avg 3.7W, total 124.2W
  rpi3: 36 nodes, avg 1.7W, total 59.7W

‚ö° TOTAL ENERGY CONSUMPTION:
Simulation duration: 0.17 hours
HPST: 64.5 Wh
LPLT: 65.8 Wh
Energy savings: -2.1%

üîç SANITY CHECKS:
üîç DEEP ANALYSIS: Understanding Why LPLT Compares to HPST

=== NODE DISTRIBUTION ANALYSIS ===
HPST uses 98 unique nodes
LPLT uses 92 unique nodes
Node spreading difference: 6

HPST node load distribution:
  Average replicas per node: 11.7
  Max replicas on one node: 40
  Min replicas on one node: 4

LPLT node load distribution:
  Average replicas per node: 11.1
  Max replicas on one node: 45
  Min replicas on one node: 4

Consolidation factor: 0.95
‚û°Ô∏è Similar distribution patterns
=== COLD START ANALYSIS ===
HPST scheduling success rate: 0.966804979253112
LPLT scheduling success rate: 1.0

Cold start execution times:
HPST early median t_exec: 1.599s
LPLT early median t_exec: 3.629s

Total scheduling events:
HPST: 724 scheduling events
LPLT: 622 scheduling events
=== WORKLOAD-SPECIFIC PERFORMANCE ===

resnet50-inference (2512 vs 2512 samples):
  Median: HPST=0.678s, LPLT=0.693s (+2.1%)
  P95: HPST=1.764s, LPLT=6.246s (+254.1%)

fio (2684 vs 2489 samples):
  Median: HPST=192.475s, LPLT=193.556s (+0.6%)
  P95: HPST=481.767s, LPLT=487.738s (+1.2%)

speech-inference (706 vs 706 samples):
  Median: HPST=9.940s, LPLT=54.620s (+449.5%)
  P95: HPST=104.052s, LPLT=200.102s (+92.3%)

python-pi (6447 vs 6447 samples):
  Median: HPST=0.924s, LPLT=0.924s (+0.0%)
  P95: HPST=25.307s, LPLT=25.109s (-0.8%)

resnet50-training (116 vs 120 samples):
  Median: HPST=282.602s, LPLT=286.012s (+1.2%)
  P95: HPST=551.611s, LPLT=525.212s (-4.8%)
=== SCALING DECISION ANALYSIS ===
HPST scaling actions:
  Scale up: 228
  Scale down: 432
  No action: 65
  Total actions: 660

LPLT scaling actions:
  Scale up: 202
  Scale down: 446
  No action: 76
  Total actions: 648

High response time events (>1s):
HPST: 44 events
LPLT: 50 events

Node type selection frequency:
HPST preferences:
  nuc: 181
  nx: 47
LPLT preferences:
  rockpi: 88
  rpi3: 52
  nano: 40
  nx: 22
=== RESOURCE CONTENTION ANALYSIS ===
HPST average utilization by node type:
           cpu_util  memory_util
node_type                       
coral      0.012446     0.010372
nano       0.017372     0.013029
nuc        0.087339     0.072782
nx         0.024262     0.017469
rockpi     0.051722     0.043102
rpi3       0.050402     0.042002

LPLT average utilization by node type:
           cpu_util  memory_util
node_type                       
coral      0.028364     0.023636
nano       0.018398     0.013799
nuc        0.120000     0.100000
nx         0.025030     0.018021
rockpi     0.061698     0.051415
rpi3       0.062782     0.052319

High CPU utilization events (>90%):
HPST: 0 events
LPLT: 0 events

Average power per active node:
HPST: 3.20W
LPLT: 3.27W

==================================================
HYPOTHESIS VALIDATION SUMMARY
==================================================
‚úÖ H3: LPLT scales more conservatively (less churn)

============================================================
üîç DETAILED WORKLOAD ANALYSIS VALIDATION
==================================================

üìä HPST Detailed Breakdown:
Workload distribution by node type:
  rockpi: 365 replicas √ó 3.58W = 1306.4W
  rpi3: 318 replicas √ó 1.61W = 511.3W
  nano: 242 replicas √ó 1.99W = 481.1W
  nx: 195 replicas √ó 7.93W = 1547.3W
  coral: 24 replicas √ó 2.45W = 58.8W
  nuc: 5 replicas √ó 8.74W = 43.7W

HPST Summary:
  Total replicas: 1149
  Total workload power: 3816.4W
  Average efficiency: 3.322W per replica
  Unique nodes used: 12

üìä LPLT Detailed Breakdown:
Workload distribution by node type:
  rockpi: 329 replicas √ó 3.65W = 1201.7W
  rpi3: 263 replicas √ó 1.66W = 436.3W
  nano: 228 replicas √ó 1.99W = 454.5W
  nx: 173 replicas √ó 7.95W = 1376.2W
  coral: 30 replicas √ó 2.52W = 75.5W

LPLT Summary:
  Total replicas: 1023
  Total workload power: 3409.6W
  Average efficiency: 3.333W per replica
  Unique nodes used: 10

üéØ COMPARATIVE ANALYSIS:
Workload efficiency improvement: -0.3%
Total workload power reduction: 10.7%
Replica count difference: 1149 vs 1023

‚úÖ VALIDATION CHECKS:
‚úó HPST has better energy efficiency per replica
‚úì LPLT uses less total workload power

=== SUMMARY ===
Infrastructure power savings: -2.1%
Real infrastructure power savings: -2.1%
Performance trade-off: 79.5% response time penalty
Plot saved as tradeoff_plot.png
