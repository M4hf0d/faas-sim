=== RUNNING HYPOTHESIS TEST ===
Hypothesis: Kubernetes reduces power consumption vs LPLT baseline, but increases response times

=== POWER CONSUMPTION COMPARISON ===

üîã LPLT Workload Analysis:
  coral_0: 4 replicas √ó 2.52W = 10.1W
  coral_1: 110 replicas √ó 2.53W = 278.0W
  coral_2: 9 replicas √ó 2.53W = 22.7W
  coral_3: 4 replicas √ó 2.53W = 10.1W
  nano_0: 4 replicas √ó 1.91W = 7.6W
  nano_1: 4 replicas √ó 1.90W = 7.6W
  nano_15: 4 replicas √ó 1.91W = 7.6W
  nano_17: 4 replicas √ó 1.91W = 7.6W
  nx_0: 8 replicas √ó 5.01W = 40.1W
  nx_10: 5 replicas √ó 8.09W = 40.4W
  nx_12: 78 replicas √ó 5.00W = 390.0W
  rockpi_11: 193 replicas √ó 3.04W = 587.7W
  rockpi_16: 8 replicas √ó 3.00W = 24.0W
  rockpi_23: 8 replicas √ó 3.00W = 24.0W
  rockpi_24: 40 replicas √ó 3.08W = 123.2W
  rockpi_25: 8 replicas √ó 3.00W = 24.0W
  rockpi_30: 8 replicas √ó 3.00W = 24.0W
  rockpi_33: 78 replicas √ó 3.04W = 237.5W
  rockpi_4: 8 replicas √ó 3.04W = 24.3W
  rockpi_5: 104 replicas √ó 3.08W = 320.4W
  rpi3_0: 8 replicas √ó 2.16W = 17.3W
  rpi3_1: 8 replicas √ó 2.18W = 17.4W
  rpi3_10: 8 replicas √ó 2.27W = 18.2W
  rpi3_11: 8 replicas √ó 2.29W = 18.3W
  rpi3_15: 8 replicas √ó 2.21W = 17.7W
  rpi3_16: 8 replicas √ó 2.21W = 17.7W
  rpi3_17: 8 replicas √ó 2.23W = 17.8W
  rpi3_18: 8 replicas √ó 2.37W = 19.0W
  rpi3_19: 8 replicas √ó 2.38W = 19.0W
  rpi3_2: 8 replicas √ó 2.19W = 17.6W
  rpi3_20: 8 replicas √ó 2.39W = 19.1W
  rpi3_21: 8 replicas √ó 2.10W = 16.8W
  rpi3_22: 8 replicas √ó 2.11W = 16.8W
  rpi3_23: 8 replicas √ó 2.12W = 16.9W
  rpi3_24: 8 replicas √ó 2.40W = 19.2W
  rpi3_25: 8 replicas √ó 2.41W = 19.3W
  rpi3_27: 4 replicas √ó 2.12W = 8.5W
  rpi3_28: 4 replicas √ó 2.12W = 8.5W
  rpi3_29: 4 replicas √ó 2.12W = 8.5W
  rpi3_3: 8 replicas √ó 2.13W = 17.1W
  rpi3_33: 8 replicas √ó 2.30W = 18.4W
  rpi3_34: 8 replicas √ó 2.32W = 18.6W
  rpi3_35: 8 replicas √ó 2.36W = 18.8W
  rpi3_4: 8 replicas √ó 2.14W = 17.1W
  rpi3_5: 8 replicas √ó 2.15W = 17.2W
  rpi3_9: 8 replicas √ó 2.25W = 18.0W
  Total: 2649.9W across 885 replicas = 2.99W/replica

üîã Kubernetes Workload Analysis:
  coral_0: 4 replicas √ó 2.52W = 10.1W
  coral_1: 119 replicas √ó 2.53W = 300.7W
  coral_2: 9 replicas √ó 2.53W = 22.7W
  coral_3: 139 replicas √ó 2.53W = 351.2W
  nano_0: 4 replicas √ó 1.91W = 7.6W
  nano_1: 4 replicas √ó 1.90W = 7.6W
  nano_12: 4 replicas √ó 1.96W = 7.9W
  nano_15: 4 replicas √ó 1.91W = 7.6W
  nano_17: 4 replicas √ó 1.91W = 7.6W
  nano_2: 4 replicas √ó 1.96W = 7.9W
  nano_3: 80 replicas √ó 2.02W = 161.9W
  nano_4: 79 replicas √ó 2.02W = 159.9W
  nano_5: 80 replicas √ó 2.02W = 161.9W
  nano_7: 84 replicas √ó 1.97W = 165.5W
  nano_8: 79 replicas √ó 2.02W = 159.9W
  nx_0: 8 replicas √ó 5.01W = 40.1W
  nx_10: 68 replicas √ó 5.38W = 365.6W
  nx_12: 88 replicas √ó 5.00W = 440.0W
  nx_13: 4 replicas √ó 5.38W = 21.5W
  nx_14: 84 replicas √ó 5.42W = 455.2W
  nx_4: 84 replicas √ó 5.41W = 454.5W
  nx_7: 84 replicas √ó 5.42W = 455.2W
  rockpi_11: 228 replicas √ó 3.04W = 694.3W
  rockpi_16: 8 replicas √ó 3.00W = 24.0W
  rockpi_21: 118 replicas √ó 3.08W = 363.4W
  rockpi_23: 8 replicas √ó 3.00W = 24.0W
  rockpi_24: 143 replicas √ó 3.08W = 440.4W
  rockpi_25: 8 replicas √ó 3.00W = 24.0W
  rockpi_27: 40 replicas √ó 3.25W = 130.2W
  rockpi_30: 8 replicas √ó 3.00W = 24.0W
  rockpi_33: 168 replicas √ó 3.04W = 511.6W
  rockpi_4: 8 replicas √ó 3.04W = 24.3W
  rockpi_5: 158 replicas √ó 3.08W = 485.9W
  rpi3_21: 8 replicas √ó 2.10W = 16.8W
  rpi3_22: 4 replicas √ó 2.12W = 8.5W
  rpi3_23: 4 replicas √ó 2.12W = 8.5W
  rpi3_27: 4 replicas √ó 2.12W = 8.5W
  rpi3_28: 4 replicas √ó 2.12W = 8.5W
  rpi3_29: 4 replicas √ó 2.12W = 8.5W
  Total: 6577.5W across 2040 replicas = 3.22W/replica
üìä TOTAL INFRASTRUCTURE POWER:
LPLT average total system power: 544.8W
Kubernetes average total system power: 519.4W
Infrastructure power savings: 4.7%

‚ö° WORKLOAD ENERGY EFFICIENCY:
LPLT: 2.99W per replica
Kubernetes: 3.22W per replica
Workload efficiency improvement: -7.7%
‚ùå LPLT is more energy-efficient per replica

=== PERFORMANCE COMPARISON ===
LPLT MEDIAN response time: 1180.832s
Kubernetes MEDIAN response time: 1121.010s
MEDIAN performance penalty: -5.1%
LPLT 95th percentile: 4909.099s
Kubernetes 95th percentile: 4485.887s
95th percentile penalty: -8.6%
LPLT warm-up avg: 11809.096s
Kubernetes warm-up avg: 10310.931s
Warm-up performance penalty: -12.7%

=== WAIT TIME ANALYSIS ===
LPLT median wait time: 0.000s
Kubernetes median wait time: 0.000s
Wait time improvement: 0.0%

=== REVISED HYPOTHESIS RESULT ===
üéâ UNEXPECTED RESULT: Kubernetes saves 4.7% infrastructure power
   and -7.7% workload efficiency
   AND improves median response times by 5.1%!
   This suggests Kubernetes strategy is superior in both dimensions
üîç DEEP ANALYSIS: Understanding Why Kubernetes Compares to LPLT

=== NODE DISTRIBUTION ANALYSIS ===
LPLT uses 46 unique nodes
Kubernetes uses 39 unique nodes
Node spreading difference: 7

LPLT node load distribution:
  Average replicas per node: 19.2
  Max replicas on one node: 193
  Min replicas on one node: 4

Kubernetes node load distribution:
  Average replicas per node: 52.3
  Max replicas on one node: 228
  Min replicas on one node: 4

Consolidation factor: 2.72
‚úÖ Kubernetes is consolidating workload (higher density per node)
=== COLD START ANALYSIS ===
LPLT scheduling success rate: 1.0
Kubernetes scheduling success rate: 1.0

Cold start execution times:
LPLT early median t_exec: 0.911s
Kubernetes early median t_exec: 0.911s

Total scheduling events:
LPLT: 576 scheduling events
Kubernetes: 1254 scheduling events
=== WORKLOAD-SPECIFIC PERFORMANCE ===

resnet50-inference (7024 vs 7026 samples):
  Median: LPLT=0.696s, Kubernetes=0.681s (-2.2%)
  P95: LPLT=1.020s, Kubernetes=0.833s (-18.3%)

fio (2035 vs 168 samples):
  Median: LPLT=148.692s, Kubernetes=285.719s (+92.2%)
  P95: LPLT=436.608s, Kubernetes=527.761s (+20.9%)

speech-inference (1539 vs 1526 samples):
  Median: LPLT=3.880s, Kubernetes=3.886s (+0.2%)
  P95: LPLT=179.797s, Kubernetes=188.396s (+4.8%)

python-pi (9157 vs 9155 samples):
  Median: LPLT=0.917s, Kubernetes=0.921s (+0.4%)
  P95: LPLT=2.003s, Kubernetes=2.149s (+7.3%)

resnet50-training (19 vs 18 samples):
  Median: LPLT=434.228s, Kubernetes=433.548s (-0.2%)
  P95: LPLT=581.662s, Kubernetes=578.474s (-0.5%)
=== SCALING DECISION ANALYSIS ===
LPLT scaling actions:
  Scale up: 175
  Scale down: 482
  No action: 1406
  Total actions: 657

Kubernetes scaling actions:
  Scale up: 401
  Scale down: 1064
  No action: 594
  Total actions: 1465

High response time events (>1s):
LPLT: 200 events
Kubernetes: 231 events

Node type selection frequency:
LPLT preferences:
  rpi3: 175
Kubernetes preferences:
  nuc: 302
  rockpi: 65
  nx: 34
=== RESOURCE CONTENTION ANALYSIS ===
LPLT average utilization by node type:
           cpu_util  memory_util
node_type                       
coral      0.030964     0.025803
nano       0.160197     0.120148
nuc        0.120000     0.100000
nx         0.209891     0.151121
rockpi     0.092369     0.076974
rpi3       0.069328     0.057773

Kubernetes average utilization by node type:
           cpu_util  memory_util
node_type                       
coral      0.031003     0.025836
nano       0.097934     0.073451
nuc        0.120000     0.100000
nx         0.139754     0.100623
rockpi     0.087631     0.073026
rpi3       0.101115     0.084262

High CPU utilization events (>90%):
LPLT: 0 events
Kubernetes: 0 events

Average power per active node:
LPLT: 4.58W
Kubernetes: 4.36W

==================================================
HYPOTHESIS VALIDATION SUMMARY
==================================================
‚úÖ H1: Kubernetes consolidates workload better than LPLT

============================================================
üîç DETAILED WORKLOAD ANALYSIS VALIDATION
==================================================

üìä LPLT Detailed Breakdown:
Workload distribution by node type:
  rockpi: 455 replicas √ó 3.47W = 1578.6W
  rpi3: 196 replicas √ó 2.28W = 447.6W
  coral: 127 replicas √ó 2.59W = 329.3W
  nx: 91 replicas √ó 9.38W = 853.8W
  nano: 16 replicas √ó 2.62W = 41.9W

LPLT Summary:
  Total replicas: 885
  Total workload power: 2649.9W
  Average efficiency: 2.994W per replica
  Unique nodes used: 9

üìä Kubernetes Detailed Breakdown:
Workload distribution by node type:
  rockpi: 895 replicas √ó 3.45W = 3083.7W
  nano: 426 replicas √ó 2.34W = 997.1W
  nx: 420 replicas √ó 7.92W = 3325.6W
  coral: 271 replicas √ó 2.59W = 702.7W
  rpi3: 28 replicas √ó 2.37W = 66.3W

Kubernetes Summary:
  Total replicas: 2040
  Total workload power: 6577.5W
  Average efficiency: 3.224W per replica
  Unique nodes used: 16

üéØ COMPARATIVE ANALYSIS:
Workload efficiency improvement: -7.7%
Total workload power reduction: -148.2%
Replica count difference: 885 vs 2040

‚úÖ VALIDATION CHECKS:
‚úó LPLT has better energy efficiency per replica
‚úó LPLT uses less total workload power

=== SUMMARY ===
Infrastructure power savings: 4.7%
Workload efficiency improvement: -7.7%
Performance trade-off: -5.1% response time penalty
Plot saved as tradeoff_plot.png
