=== RUNNING HYPOTHESIS TEST ===
Hypothesis: HPST reduces power consumption vs Kubernetes baseline, but increases response times

=== POWER CONSUMPTION COMPARISON ===
 TOTAL INFRASTRUCTURE POWER:
Kubernetes average total system power: 507.3W
HPST average total system power: 458.7W
Infrastructure power savings: 9.6%

=== PERFORMANCE COMPARISON ===
Kubernetes MEDIAN response time: 1258.138s
HPST MEDIAN response time: 1053.874s
MEDIAN performance penalty: -16.2%
Kubernetes 95th percentile: 124885.649s
HPST 95th percentile: 97195.389s
95th percentile penalty: -22.2%
Kubernetes warm-up avg: 17355.714s
HPST warm-up avg: 12003.922s
Warm-up performance penalty: -30.8%

=== WAIT TIME ANALYSIS ===
Kubernetes median wait time: 0.000s
HPST median wait time: 0.000s
Wait time improvement: 0.0%

=== REVISED HYPOTHESIS RESULT ===
üéâ UNEXPECTED RESULT: HPST saves 9.6% infrastructure power
   AND improves median response times by 16.2%!
   This suggests HPST strategy is superior in both dimensions

============================================================
üîã REAL INFRASTRUCTURE POWER ANALYSIS (FIXED)
==================================================
HPST power data: 72600 measurements
LPLT power data: 72600 measurements
HPST unique nodes: 120
LPLT unique nodes: 120
HPST time range: 1.0 - 605.0
LPLT time range: 1.0 - 605.0

üìä  SYSTEM POWER:
Kubernetes average total system power: 507.3W
HPST average total system power: 458.7W
Infrastructure power savings: 9.6%

üìä POWER DISTRIBUTION:

Kubernetes power by node type:
  coral: 4 nodes, avg 2.7W, total 10.6W
  nano: 29 nodes, avg 2.9W, total 83.5W
  nuc: 3 nodes, avg 9.8W, total 29.3W
  nx: 14 nodes, avg 13.0W, total 181.6W
  rockpi: 34 nodes, avg 4.0W, total 135.5W
  rpi3: 36 nodes, avg 1.9W, total 66.8W

HPST power by node type:
  coral: 4 nodes, avg 2.7W, total 10.6W
  nano: 29 nodes, avg 2.5W, total 71.7W
  nuc: 3 nodes, avg 9.8W, total 29.3W
  nx: 14 nodes, avg 11.2W, total 157.3W
  rockpi: 34 nodes, avg 3.8W, total 127.7W
  rpi3: 36 nodes, avg 1.7W, total 62.1W

‚ö° TOTAL ENERGY CONSUMPTION:
Simulation duration: 0.17 hours
Kubernetes: 85.1 Wh
HPST: 77.0 Wh
Energy savings: 9.6%

üîç SANITY CHECKS:
üîç DEEP ANALYSIS: Understanding Why HPST Compares to Kubernetes

=== NODE DISTRIBUTION ANALYSIS ===
Kubernetes uses 12 unique nodes
HPST uses 53 unique nodes
Node spreading difference: -41

Kubernetes node load distribution:
  Average replicas per node: 15.9
  Max replicas on one node: 35
  Min replicas on one node: 4

HPST node load distribution:
  Average replicas per node: 12.8
  Max replicas on one node: 30
  Min replicas on one node: 4

Consolidation factor: 0.80
‚û°Ô∏è Similar distribution patterns
=== COLD START ANALYSIS ===
Kubernetes scheduling success rate: 1.0
HPST scheduling success rate: 1.0

Cold start execution times:
Kubernetes early median t_exec: 0.908s
HPST early median t_exec: 0.909s

Total scheduling events:
Kubernetes: 120 scheduling events
HPST: 414 scheduling events
=== WORKLOAD-SPECIFIC PERFORMANCE ===

resnet50-inference (1254 vs 1254 samples):
  Median: Kubernetes=0.382s, HPST=0.667s (+74.8%)
  P95: Kubernetes=0.414s, HPST=0.786s (+89.9%)

fio (87 vs 1372 samples):
  Median: Kubernetes=295.449s, HPST=134.533s (-54.5%)
  P95: Kubernetes=557.337s, HPST=431.567s (-22.6%)

speech-inference (181 vs 181 samples):
  Median: Kubernetes=16.948s, HPST=4.181s (-75.3%)
  P95: Kubernetes=27.977s, HPST=22.140s (-20.9%)

python-pi (1898 vs 1898 samples):
  Median: Kubernetes=0.949s, HPST=0.917s (-3.4%)
  P95: Kubernetes=1.965s, HPST=1.914s (-2.6%)
=== SCALING DECISION ANALYSIS ===
Kubernetes scaling actions:
  Scale up: 34
  Scale down: 280
  No action: 412
  Total actions: 314

HPST scaling actions:
  Scale up: 132
  Scale down: 412
  No action: 182
  Total actions: 544

High response time events (>1s):
Kubernetes: 27 events
HPST: 28 events

Node type selection frequency:
Kubernetes preferences:
  rockpi: 28
  nuc: 6
HPST preferences:
  nuc: 93
  nx: 39
=== RESOURCE CONTENTION ANALYSIS ===
Kubernetes average utilization by node type:
           cpu_util  memory_util
node_type                       
coral      0.063967     0.053306
nano       0.193103     0.144828
nuc        0.120000     0.100000
nx         0.216706     0.156028
rockpi     0.107002     0.089169
rpi3       0.110496     0.092080

HPST average utilization by node type:
           cpu_util  memory_util
node_type                       
coral      0.061736     0.051446
nano       0.112887     0.084665
nuc        0.120000     0.100000
nx         0.150413     0.108298
rockpi     0.075617     0.063014
rpi3       0.079036     0.065863

High CPU utilization events (>90%):
Kubernetes: 0 events
HPST: 0 events

Average power per active node:
Kubernetes: 4.23W
HPST: 3.82W

==================================================
HYPOTHESIS VALIDATION SUMMARY
==================================================

============================================================
üîç DETAILED WORKLOAD ANALYSIS VALIDATION
==================================================

üìä Kubernetes Detailed Breakdown:
Workload distribution by node type:
  nx: 64 replicas √ó 12.97W = 830.0W
  rpi3: 54 replicas √ó 1.86W = 100.2W
  rockpi: 40 replicas √ó 3.98W = 159.4W
  coral: 29 replicas √ó 2.66W = 77.2W
  nano: 4 replicas √ó 2.88W = 11.5W

Kubernetes Summary:
  Total replicas: 191
  Total workload power: 780.4W
  Average efficiency: 4.086W per replica
  Unique nodes used: 9

üìä HPST Detailed Breakdown:
Workload distribution by node type:
  rockpi: 236 replicas √ó 3.75W = 886.1W
  rpi3: 203 replicas √ó 1.73W = 350.4W
  nano: 119 replicas √ó 2.47W = 294.2W
  nx: 111 replicas √ó 11.23W = 1247.1W
  coral: 10 replicas √ó 2.65W = 26.5W

HPST Summary:
  Total replicas: 679
  Total workload power: 2229.9W
  Average efficiency: 3.284W per replica
  Unique nodes used: 10

üéØ COMPARATIVE ANALYSIS:
Workload efficiency improvement: 19.6%
Total workload power reduction: -185.8%
Replica count difference: 191 vs 679

‚úÖ VALIDATION CHECKS:
‚úì HPST has better energy efficiency per replica
‚úó Kubernetes uses less total workload power

=== SUMMARY ===
Infrastructure power savings: 9.6%
Real infrastructure power savings: 9.6%
Performance trade-off: -16.2% response time penalty
Plot saved as tradeoff_plot.png
