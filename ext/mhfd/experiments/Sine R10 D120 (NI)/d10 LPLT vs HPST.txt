=== RUNNING HYPOTHESIS TEST ===
Hypothesis: HPST reduces power consumption vs LPLT baseline, but increases response times

=== POWER CONSUMPTION COMPARISON ===
 TOTAL INFRASTRUCTURE POWER:
LPLT average total system power: 487.9W
HPST average total system power: 458.7W
Infrastructure power savings: 6.0%

=== PERFORMANCE COMPARISON ===
LPLT MEDIAN response time: 1187.343s
HPST MEDIAN response time: 1053.874s
MEDIAN performance penalty: -11.2%
LPLT 95th percentile: 124885.649s
HPST 95th percentile: 97195.389s
95th percentile penalty: -22.2%
LPLT warm-up avg: 16601.658s
HPST warm-up avg: 12003.922s
Warm-up performance penalty: -27.7%

=== WAIT TIME ANALYSIS ===
LPLT median wait time: 0.000s
HPST median wait time: 0.000s
Wait time improvement: 0.0%

=== REVISED HYPOTHESIS RESULT ===
üéâ UNEXPECTED RESULT: HPST saves 6.0% infrastructure power
   AND improves median response times by 11.2%!
   This suggests HPST strategy is superior in both dimensions

============================================================
üîã REAL INFRASTRUCTURE POWER ANALYSIS (FIXED)
==================================================
HPST power data: 72600 measurements
LPLT power data: 72600 measurements
HPST unique nodes: 120
LPLT unique nodes: 120
HPST time range: 1.0 - 605.0
LPLT time range: 1.0 - 605.0

üìä  SYSTEM POWER:
LPLT average total system power: 487.9W
HPST average total system power: 458.7W
Infrastructure power savings: 6.0%

üìä POWER DISTRIBUTION:

LPLT power by node type:
  coral: 4 nodes, avg 2.7W, total 10.6W
  nano: 29 nodes, avg 2.9W, total 83.5W
  nuc: 3 nodes, avg 9.8W, total 29.3W
  nx: 14 nodes, avg 12.1W, total 168.9W
  rockpi: 34 nodes, avg 3.9W, total 131.3W
  rpi3: 36 nodes, avg 1.8W, total 64.3W

HPST power by node type:
  coral: 4 nodes, avg 2.7W, total 10.6W
  nano: 29 nodes, avg 2.5W, total 71.7W
  nuc: 3 nodes, avg 9.8W, total 29.3W
  nx: 14 nodes, avg 11.2W, total 157.3W
  rockpi: 34 nodes, avg 3.8W, total 127.7W
  rpi3: 36 nodes, avg 1.7W, total 62.1W

‚ö° TOTAL ENERGY CONSUMPTION:
Simulation duration: 0.17 hours
LPLT: 81.9 Wh
HPST: 77.0 Wh
Energy savings: 6.0%

üîç SANITY CHECKS:
üîç DEEP ANALYSIS: Understanding Why HPST Compares to LPLT

=== NODE DISTRIBUTION ANALYSIS ===
LPLT uses 27 unique nodes
HPST uses 53 unique nodes
Node spreading difference: -26

LPLT node load distribution:
  Average replicas per node: 14.8
  Max replicas on one node: 35
  Min replicas on one node: 4

HPST node load distribution:
  Average replicas per node: 12.8
  Max replicas on one node: 30
  Min replicas on one node: 4

Consolidation factor: 0.86
‚û°Ô∏è Similar distribution patterns
=== COLD START ANALYSIS ===
LPLT scheduling success rate: 1.0
HPST scheduling success rate: 1.0

Cold start execution times:
LPLT early median t_exec: 0.908s
HPST early median t_exec: 0.909s

Total scheduling events:
LPLT: 246 scheduling events
HPST: 414 scheduling events
=== WORKLOAD-SPECIFIC PERFORMANCE ===

resnet50-inference (1254 vs 1254 samples):
  Median: LPLT=0.383s, HPST=0.667s (+74.4%)
  P95: LPLT=0.419s, HPST=0.786s (+87.6%)

fio (1126 vs 1372 samples):
  Median: LPLT=153.501s, HPST=134.533s (-12.4%)
  P95: LPLT=442.662s, HPST=431.567s (-2.5%)

speech-inference (181 vs 181 samples):
  Median: LPLT=6.777s, HPST=4.181s (-38.3%)
  P95: LPLT=22.719s, HPST=22.140s (-2.5%)

python-pi (1898 vs 1898 samples):
  Median: LPLT=0.947s, HPST=0.917s (-3.2%)
  P95: LPLT=1.932s, HPST=1.914s (-0.9%)
=== SCALING DECISION ANALYSIS ===
LPLT scaling actions:
  Scale up: 76
  Scale down: 345
  No action: 305
  Total actions: 421

HPST scaling actions:
  Scale up: 132
  Scale down: 412
  No action: 182
  Total actions: 544

High response time events (>1s):
LPLT: 29 events
HPST: 28 events

Node type selection frequency:
LPLT preferences:
  rockpi: 52
  rpi3: 24
HPST preferences:
  nuc: 93
  nx: 39
=== RESOURCE CONTENTION ANALYSIS ===
LPLT average utilization by node type:
           cpu_util  memory_util
node_type                       
coral      0.062727     0.052273
nano       0.193103     0.144828
nuc        0.120000     0.100000
nx         0.182025     0.131058
rockpi     0.090411     0.075343
rpi3       0.093526     0.077938

HPST average utilization by node type:
           cpu_util  memory_util
node_type                       
coral      0.061736     0.051446
nano       0.112887     0.084665
nuc        0.120000     0.100000
nx         0.150413     0.108298
rockpi     0.075617     0.063014
rpi3       0.079036     0.065863

High CPU utilization events (>90%):
LPLT: 0 events
HPST: 0 events

Average power per active node:
LPLT: 4.07W
HPST: 3.82W

==================================================
HYPOTHESIS VALIDATION SUMMARY
==================================================

============================================================
üîç DETAILED WORKLOAD ANALYSIS VALIDATION
==================================================

üìä LPLT Detailed Breakdown:
Workload distribution by node type:
  rockpi: 174 replicas √ó 3.86W = 672.2W
  rpi3: 138 replicas √ó 1.79W = 246.4W
  nx: 74 replicas √ó 12.06W = 892.6W
  coral: 10 replicas √ó 2.66W = 26.6W
  nano: 4 replicas √ó 2.88W = 11.5W

LPLT Summary:
  Total replicas: 400
  Total workload power: 1460.2W
  Average efficiency: 3.651W per replica
  Unique nodes used: 11

üìä HPST Detailed Breakdown:
Workload distribution by node type:
  rockpi: 236 replicas √ó 3.75W = 886.1W
  rpi3: 203 replicas √ó 1.73W = 350.4W
  nano: 119 replicas √ó 2.47W = 294.2W
  nx: 111 replicas √ó 11.23W = 1247.1W
  coral: 10 replicas √ó 2.65W = 26.5W

HPST Summary:
  Total replicas: 679
  Total workload power: 2229.9W
  Average efficiency: 3.284W per replica
  Unique nodes used: 10

üéØ COMPARATIVE ANALYSIS:
Workload efficiency improvement: 10.0%
Total workload power reduction: -52.7%
Replica count difference: 400 vs 679

‚úÖ VALIDATION CHECKS:
‚úì HPST has better energy efficiency per replica
‚úó LPLT uses less total workload power

=== SUMMARY ===
Infrastructure power savings: 6.0%
Real infrastructure power savings: 6.0%
Performance trade-off: -11.2% response time penalty
Plot saved as tradeoff_plot.png
