=== RUNNING HYPOTHESIS TEST ===
Hypothesis: Kubernetes reduces power consumption vs HPST baseline, but increases response times

=== POWER CONSUMPTION COMPARISON ===
 TOTAL INFRASTRUCTURE POWER:
HPST average total system power: 1633.3W
Kubernetes average total system power: 1751.0W
Infrastructure power savings: -7.2%

=== PERFORMANCE COMPARISON ===
HPST MEDIAN response time: 2987.413s
Kubernetes MEDIAN response time: 10085.050s
MEDIAN performance penalty: 237.6%
HPST 95th percentile: 109138.382s
Kubernetes 95th percentile: 291014.497s
95th percentile penalty: 166.6%
HPST warm-up avg: 19769.028s
Kubernetes warm-up avg: 59796.966s
Warm-up performance penalty: 202.5%

=== WAIT TIME ANALYSIS ===
HPST median wait time: 0.000s
Kubernetes median wait time: 9082.166s
Wait time improvement: 0.0%

=== REVISED HYPOTHESIS RESULT ===
‚ùå HYPOTHESIS NOT CONFIRMED
   Power savings: -7.2%

============================================================
üîã REAL INFRASTRUCTURE POWER ANALYSIS (FIXED)
==================================================
HPST power data: 304500 measurements
LPLT power data: 304500 measurements
HPST unique nodes: 500
LPLT unique nodes: 500
HPST time range: 1.0 - 609.0
LPLT time range: 1.0 - 609.0

üìä  SYSTEM POWER:
HPST average total system power: 1633.3W
Kubernetes average total system power: 1751.0W
Infrastructure power savings: -7.2%

üìä POWER DISTRIBUTION:

HPST power by node type:
  coral: 34 nodes, avg 2.7W, total 92.2W
  nano: 113 nodes, avg 2.2W, total 253.9W
  nuc: 3 nodes, avg 9.8W, total 29.3W
  nx: 53 nodes, avg 8.7W, total 459.2W
  rockpi: 147 nodes, avg 3.7W, total 544.4W
  rpi3: 150 nodes, avg 1.7W, total 254.4W

Kubernetes power by node type:
  coral: 34 nodes, avg 2.8W, total 94.2W
  nano: 113 nodes, avg 2.4W, total 270.3W
  nuc: 3 nodes, avg 9.8W, total 29.3W
  nx: 53 nodes, avg 9.7W, total 516.5W
  rockpi: 147 nodes, avg 3.9W, total 570.4W
  rpi3: 150 nodes, avg 1.8W, total 270.3W

‚ö° TOTAL ENERGY CONSUMPTION:
Simulation duration: 0.17 hours
HPST: 275.9 Wh
Kubernetes: 295.7 Wh
Energy savings: -7.2%

üîç SANITY CHECKS:
‚ö†Ô∏è WARNING: HPST power (1633.3W) outside expected range (200, 800)
‚ö†Ô∏è WARNING: Kubernetes power (1751.0W) outside expected range (200, 800)
üîç DEEP ANALYSIS: Understanding Why Kubernetes Compares to HPST

=== NODE DISTRIBUTION ANALYSIS ===
HPST uses 396 unique nodes
Kubernetes uses 197 unique nodes
Node spreading difference: 199

HPST node load distribution:
  Average replicas per node: 12.9
  Max replicas on one node: 38
  Min replicas on one node: 3

Kubernetes node load distribution:
  Average replicas per node: 16.3
  Max replicas on one node: 68
  Min replicas on one node: 4

Consolidation factor: 1.26
‚úÖ Kubernetes is consolidating workload (higher density per node)
=== COLD START ANALYSIS ===
HPST scheduling success rate: 0.9913569576490925
Kubernetes scheduling success rate: 1.0

Cold start execution times:
HPST early median t_exec: 6.950s
Kubernetes early median t_exec: 7.813s

Total scheduling events:
HPST: 3471 scheduling events
Kubernetes: 2061 scheduling events
=== WORKLOAD-SPECIFIC PERFORMANCE ===

resnet50-inference (24788 vs 24792 samples):
  Median: HPST=0.711s, Kubernetes=0.720s (+1.3%)
  P95: HPST=94.074s, Kubernetes=100.108s (+6.4%)

fio (5602 vs 2964 samples):
  Median: HPST=164.107s, Kubernetes=192.978s (+17.6%)
  P95: HPST=469.624s, Kubernetes=498.993s (+6.3%)

speech-inference (7045 vs 4047 samples):
  Median: HPST=3.876s, Kubernetes=49.641s (+1180.7%)
  P95: HPST=196.518s, Kubernetes=376.246s (+91.5%)

python-pi (30929 vs 30932 samples):
  Median: HPST=0.930s, Kubernetes=1.301s (+39.8%)
  P95: HPST=23.122s, Kubernetes=55.488s (+140.0%)

resnet50-training (132 vs 176 samples):
  Median: HPST=255.322s, Kubernetes=331.097s (+29.7%)
  P95: HPST=510.134s, Kubernetes=537.789s (+5.4%)
=== SCALING DECISION ANALYSIS ===
HPST scaling actions:
  Scale up: 1130
  Scale down: 720
  No action: 207
  Total actions: 1850

Kubernetes scaling actions:
  Scale up: 670
  Scale down: 844
  No action: 543
  Total actions: 1514

High response time events (>1s):
HPST: 276 events
Kubernetes: 267 events

Node type selection frequency:
HPST preferences:
  nuc: 632
  nx: 498
Kubernetes preferences:
  nuc: 523
  rockpi: 78
  nx: 69
=== RESOURCE CONTENTION ANALYSIS ===
HPST average utilization by node type:
           cpu_util  memory_util
node_type                       
coral      0.076169     0.063474
nano       0.068486     0.051364
nuc        0.120000     0.100000
nx         0.052158     0.037554
rockpi     0.068612     0.057176
rpi3       0.071707     0.059756

Kubernetes average utilization by node type:
           cpu_util  memory_util
node_type                       
coral      0.090669     0.075558
nano       0.097104     0.072828
nuc        0.120000     0.100000
nx         0.093457     0.067289
rockpi     0.092788     0.077323
rpi3       0.097389     0.081157

High CPU utilization events (>90%):
HPST: 0 events
Kubernetes: 0 events

Average power per active node:
HPST: 3.27W
Kubernetes: 3.50W

==================================================
HYPOTHESIS VALIDATION SUMMARY
==================================================
‚úÖ H1: Kubernetes consolidates workload better than HPST
‚úÖ H3: Kubernetes scales more conservatively (less churn)

============================================================
üîç DETAILED WORKLOAD ANALYSIS VALIDATION
==================================================

üìä HPST Detailed Breakdown:
Workload distribution by node type:
  rockpi: 1831 replicas √ó 3.70W = 6780.5W
  rpi3: 1138 replicas √ó 1.70W = 1929.8W
  nx: 1045 replicas √ó 8.66W = 9054.4W
  nano: 944 replicas √ó 2.25W = 2121.4W
  coral: 137 replicas √ó 2.71W = 371.4W

HPST Summary:
  Total replicas: 5095
  Total workload power: 19259.1W
  Average efficiency: 3.780W per replica
  Unique nodes used: 21

üìä Kubernetes Detailed Breakdown:
Workload distribution by node type:
  rockpi: 1055 replicas √ó 3.88W = 4093.9W
  nx: 771 replicas √ó 9.74W = 7513.3W
  nano: 713 replicas √ó 2.39W = 1705.7W
  rpi3: 497 replicas √ó 1.80W = 895.5W
  coral: 166 replicas √ó 2.77W = 459.9W

Kubernetes Summary:
  Total replicas: 3202
  Total workload power: 12300.8W
  Average efficiency: 3.842W per replica
  Unique nodes used: 24

üéØ COMPARATIVE ANALYSIS:
Workload efficiency improvement: -1.6%
Total workload power reduction: 36.1%
Replica count difference: 5095 vs 3202

‚úÖ VALIDATION CHECKS:
‚úó HPST has better energy efficiency per replica
‚úì Kubernetes uses less total workload power

=== SUMMARY ===
Infrastructure power savings: -7.2%
Real infrastructure power savings: -7.2%
Performance trade-off: 237.6% response time penalty
Plot saved as tradeoff_plot.png
