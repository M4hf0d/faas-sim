AUTOSCALER DETAILED METRICS SUMMARY
===================================

Report generated: 2025-08-07 13:53:06

Total measurement records: 508
Deployments analyzed: 11
Time range: 40.00 - 605.00s
Simulation duration: 565.00s

OVERALL STATISTICS
==================
Average response time: 23869.90 ms
Max response time: 194873.94 ms
Average execution time: 2151.73 ms
Average wait time: 21718.17 ms
Average wait percentage: 50.69%
Total high wait count events: 7108
Average sample count: 48.0

PER-DEPLOYMENT BREAKDOWN
========================

Deployment: python-pi
  Measurement count: 64
  Time range: 40.0 - 605.0s
  Avg response time: 28181.08 ms
  Min/Max response time: 1126.51/128145.39 ms
  Avg execution time: 1229.47 ms
  Avg wait time: 26951.61 ms
  Avg wait percentage: 68.11%
  High wait events: 3016
  Avg sample count: 144.6
  Performance trend: improving

Deployment: python-pi-downtown
  Measurement count: 64
  Time range: 40.0 - 605.0s
  Avg response time: 33378.10 ms
  Min/Max response time: 977.29/161641.23 ms
  Avg execution time: 1253.68 ms
  Avg wait time: 32124.42 ms
  Avg wait percentage: 70.22%
  High wait events: 1066
  Avg sample count: 47.8
  Performance trend: improving

Deployment: python-pi-suburb
  Measurement count: 64
  Time range: 40.0 - 605.0s
  Avg response time: 33054.46 ms
  Min/Max response time: 1266.40/162168.81 ms
  Avg execution time: 1307.83 ms
  Avg wait time: 31746.63 ms
  Avg wait percentage: 69.85%
  High wait events: 1064
  Avg sample count: 47.6
  Performance trend: improving

Deployment: resnet50-inference
  Measurement count: 40
  Time range: 40.0 - 605.0s
  Avg response time: 711.16 ms
  Min/Max response time: 533.36/2195.14 ms
  Avg execution time: 554.12 ms
  Avg wait time: 157.04 ms
  Avg wait percentage: 7.49%
  High wait events: 300
  Avg sample count: 116.6
  Performance trend: improving

Deployment: resnet50-inference-downtown
  Measurement count: 40
  Time range: 40.0 - 605.0s
  Avg response time: 1055.66 ms
  Min/Max response time: 626.02/4514.31 ms
  Avg execution time: 705.53 ms
  Avg wait time: 350.13 ms
  Avg wait percentage: 8.28%
  High wait events: 150
  Avg sample count: 27.7
  Performance trend: improving

Deployment: resnet50-inference-industrial
  Measurement count: 40
  Time range: 40.0 - 605.0s
  Avg response time: 1154.47 ms
  Min/Max response time: 681.14/5172.44 ms
  Avg execution time: 735.79 ms
  Avg wait time: 418.68 ms
  Avg wait percentage: 8.53%
  High wait events: 150
  Avg sample count: 27.8
  Performance trend: improving

Deployment: resnet50-inference-suburb
  Measurement count: 20
  Time range: 40.0 - 605.0s
  Avg response time: 387.86 ms
  Min/Max response time: 377.92/403.62 ms
  Avg execution time: 387.84 ms
  Avg wait time: 0.02 ms
  Avg wait percentage: 0.01%
  High wait events: 0
  Avg sample count: 30.6
  Performance trend: stable

Deployment: resnet50-preprocessing
  Measurement count: 34
  Time range: 40.0 - 600.0s
  Avg response time: 95609.46 ms
  Min/Max response time: 20877.18/194873.94 ms
  Avg execution time: 6549.41 ms
  Avg wait time: 89060.05 ms
  Avg wait percentage: 89.50%
  High wait events: 118
  Avg sample count: 3.5
  Performance trend: degrading

Deployment: speech-inference
  Measurement count: 48
  Time range: 40.0 - 535.0s
  Avg response time: 19974.26 ms
  Min/Max response time: 3754.23/46713.90 ms
  Avg execution time: 3803.21 ms
  Avg wait time: 16171.05 ms
  Avg wait percentage: 61.00%
  High wait events: 698
  Avg sample count: 16.6
  Performance trend: improving

Deployment: speech-inference-downtown
  Measurement count: 48
  Time range: 40.0 - 535.0s
  Avg response time: 18896.39 ms
  Min/Max response time: 3724.36/44916.97 ms
  Avg execution time: 3808.44 ms
  Avg wait time: 15087.95 ms
  Avg wait percentage: 60.43%
  High wait events: 276
  Avg sample count: 6.7
  Performance trend: improving

Deployment: speech-inference-suburb
  Measurement count: 46
  Time range: 40.0 - 535.0s
  Avg response time: 18032.94 ms
  Min/Max response time: 3784.11/42691.11 ms
  Avg execution time: 3800.92 ms
  Avg wait time: 14232.02 ms
  Avg wait percentage: 56.20%
  High wait events: 270
  Avg sample count: 6.8
  Performance trend: improving

PERFORMANCE ANALYSIS
===================

Records with high wait percentage (>50%): 288
  resnet50-inference: 4 records, avg 74.9% wait
  resnet50-inference-downtown: 4 records, avg 82.8% wait
  resnet50-inference-industrial: 4 records, avg 85.3% wait
  speech-inference: 34 records, avg 81.1% wait
  speech-inference-downtown: 34 records, avg 80.3% wait
  speech-inference-suburb: 30 records, avg 80.5% wait
  resnet50-preprocessing: 34 records, avg 89.5% wait
  python-pi: 48 records, avg 88.5% wait
  python-pi-downtown: 48 records, avg 90.7% wait
  python-pi-suburb: 48 records, avg 90.2% wait

Records with high response times (>75053.2ms): 50
  resnet50-preprocessing: 22 records, avg 125647.1ms
  python-pi: 8 records, avg 120300.5ms
  python-pi-downtown: 10 records, avg 132886.6ms
  python-pi-suburb: 10 records, avg 130719.8ms

SCALING RECOMMENDATIONS
======================

python-pi:
  🟡 WARNING: Moderate wait times - consider scaling up
  ⚠️  High response times detected - investigate bottlenecks

python-pi-downtown:
  🔴 CRITICAL: Very high wait times - immediate scaling needed
  ⚠️  High response times detected - investigate bottlenecks

python-pi-suburb:
  🟡 WARNING: Moderate wait times - consider scaling up
  ⚠️  High response times detected - investigate bottlenecks

resnet50-inference:
  ✅ ACCEPTABLE: Wait times within reasonable range

resnet50-inference-downtown:
  ✅ ACCEPTABLE: Wait times within reasonable range

resnet50-inference-industrial:
  ✅ ACCEPTABLE: Wait times within reasonable range

resnet50-inference-suburb:
  🟢 GOOD: Low wait times - consider scaling down if consistent

resnet50-preprocessing:
  🔴 CRITICAL: Very high wait times - immediate scaling needed
  ⚠️  High response times detected - investigate bottlenecks

speech-inference:
  🟡 WARNING: Moderate wait times - consider scaling up
  ⚠️  High response times detected - investigate bottlenecks

speech-inference-downtown:
  🟡 WARNING: Moderate wait times - consider scaling up
  ⚠️  High response times detected - investigate bottlenecks

speech-inference-suburb:
  🟡 WARNING: Moderate wait times - consider scaling up
  ⚠️  High response times detected - investigate bottlenecks

